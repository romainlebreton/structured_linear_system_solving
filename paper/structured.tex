\documentclass[sigconf]{acmart}
% \documentclass{sig-alternate}

% \usepackage{amsthm}
\usepackage{bm}
\usepackage{mathrsfs}
\usepackage{enumerate}
% \usepackage[colorlinks=true,linkcolor=cyan]{hyperref}

\usepackage[lined,boxed,ruled,vlined]{algorithm2e}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}

\author{Seung Gyu Hyun}
\affiliation{%
  \institution{Cheriton School of Computer Science}
  \institution{University of Waterloo}
}
\email{sghyun@edu.uwaterloo.ca}

\author{Romain Lebreton}
\affiliation{
  \institution{LIRMM}
  \institution{Universit\'e de Montpellier}
}
\email{romain.lebreton@lirmm.fr}

\author{\'Eric Schost}
\affiliation{%
  \institution{Cheriton School of Computer Science}
  \institution{University of Waterloo}
}
\email{eschost@uwaterloo.ca}

\newcommand{\va}{\ensuremath{\mathsf{a}}}
\newcommand{\vb}{\ensuremath{\mathsf{b}}}
\newcommand{\vc}{\ensuremath{\mathsf{c}}}
\newcommand{\ve}{\ensuremath{\mathsf{e}}}
\newcommand{\vf}{\ensuremath{\mathsf{f}}}
\newcommand{\vg}{\ensuremath{\mathsf{g}}}
\newcommand{\vh}{\ensuremath{\mathsf{h}}}
\newcommand{\vi}{\ensuremath{\mathsf{i}}}
\newcommand{\vr}{\ensuremath{\mathsf{r}}}
\newcommand{\vu}{\ensuremath{\mathsf{u}}}
\newcommand{\vv}{\ensuremath{\mathsf{v}}}
\newcommand{\vx}{\ensuremath{\mathsf{x}}}
\newcommand{\vy}{\ensuremath{\mathsf{y}}}

\newcommand{\mA}{\ensuremath{\mathsf{A}}}
\newcommand{\mB}{\ensuremath{\mathsf{B}}}
\newcommand{\mC}{\ensuremath{\mathsf{C}}}
\newcommand{\mD}{\ensuremath{\mathsf{D}}}
\newcommand{\mG}{\ensuremath{\mathsf{G}}}
\newcommand{\mH}{\ensuremath{\mathsf{H}}}
\newcommand{\mI}{\ensuremath{\mathsf{I}}}
\newcommand{\mJ}{\ensuremath{\mathsf{J}}}
\newcommand{\mM}{\ensuremath{\mathsf{M}}}
\newcommand{\mN}{\ensuremath{\mathsf{N}}}
\newcommand{\mP}{\ensuremath{\mathsf{P}}}
\newcommand{\mR}{\ensuremath{\mathsf{R}}}
\newcommand{\mS}{\ensuremath{\mathsf{S}}}
\newcommand{\mT}{\ensuremath{\mathsf{T}}}
\newcommand{\mV}{\ensuremath{\mathsf{V}}}
\newcommand{\mW}{\ensuremath{\mathsf{W}}}
\newcommand{\mX}{\ensuremath{\mathsf{X}}}
\newcommand{\mY}{\ensuremath{\mathsf{Y}}}
\newcommand{\mZ}{\ensuremath{\mathsf{Z}}}

\newcommand{\K}{\ensuremath{\mathbb{K}}}
\newcommand{\Q}{\ensuremath{\mathbb{Q}}}
\newcommand{\Z}{\ensuremath{\mathbb{Z}}}

\newcommand{\M}{\ensuremath{\mathscr{M}}}
\newcommand{\I}{\ensuremath{\mathscr{I}}}
\newcommand{\CA}{\ensuremath{\mathscr{C}_\mA}}
\newcommand{\P}{\ensuremath{\mathscr{P}}}

\newcommand{\mx}{\ensuremath{\nu}}
\newcommand{\mn}{\ensuremath{\mu}}

\newcommand{\lm}{\ensuremath{\operatorname{lm}}}
\newcommand{\rank}{\ensuremath{\operatorname{rank}}}
\newcommand{\DACp}{\ensuremath{\mathsf{DAC}_p}}
\newcommand{\DACQ}{\ensuremath{\mathsf{DAC}_\Q}}
\newcommand{\Iter}{\ensuremath{\mathsf{Iter}}}
\newcommand{\Raise}{\textbf{raise} error}


\DeclareBoldMathCommand{\bF}{F}
\DeclareBoldMathCommand{\bh}{h}
\DeclareBoldMathCommand{\bu}{u}
\DeclareBoldMathCommand{\bx}{x}
\DeclareBoldMathCommand{\by}{y}

\newcommand{\Otilde}[1]{\ensuremath{O\tilde{~}(#1)}} % soft O for complexity
\newcommand{\todo}[1]{(\textbf{todo:} #1)} 
\newcommand{\why}{\textbf{why?}} 

\newtheorem{pbm}{Problem}
\renewcommand{\thepbm}{\Alph{pbm}} % "letter-numbered" theorems
% \newtheorem{definition}{Definition}
% \newtheorem{theorem}[definition]{Theorem}
% \newtheorem{corollary}[definition]{Corollary}
% \newtheorem{proposition}[definition]{Proposition}
% \newtheorem{lemma}[definition]{Lemma}

\theoremstyle{acmdefinition}
\newtheorem{remark}[theorem]{Remark}
% \newtheorem{algo}{Algorithm}

\def\gathen#1{{#1}}

\title{Algorithms for structured linear systems and their implementation}

\begin{document}

\maketitle

\begin{abstract}
  There exists a vast literature dedicated to algorithms for
  structured matrices, but relatively few descriptions of actual
  implementations and their practical performance. In this paper, we
  consider the problem of solving Cauchy-like systems, and its
  application to mosaic Toeplitz systems, in two contexts: first in
  the unit cost model (which is a good model for computations over
  finite fields), then over $\Q$. We introduce new variants of
  previous algorithms and describe an implementation of these
  techniques and its practical behavior. We pay a special attention to
  particular cases such as the computation of algebraic approximants.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

In this paper, we discuss linear algebra algorithms inspired by the
following kind of question: given polynomials such as
\[
\left\{
\begin{array}{rcl}
t_0 &=& 8x^4 - 8x^2 + 1\\
t_1 &=& 16x^5 - 20x^3 + 5x\\
t_2 &=& 32x^6 - 48x^4 + 18x^2 - 1,
\end{array}
\right.
\]
find that the relation $t_0-2x t_1+t_2=0$ holds (this is the
recurrence between Chebyshev polynomials of the first kind).  

More precisely, given input polynomials $(t_0,\dots,t_{s-1})$ over a
field $\K$, together with integers $(n_0,\dots,n_{s-1})$ and $\sigma$,
the {\em Hermite-Pad\'e} approximation problem asks to compute
polynomials $(p_0,\dots,p_{s-1})$, not all zero, such that $\deg(p_i)
< n_i$ holds for all $i$, and such that we have $p_0 t_0 + \cdots +
p_{s-1} t_{s-1}=O(x^\sigma)$. There exist numerous applications to
this type of question, very important particular cases being {\em
  algebraic approximants} (with $t_i =f^i$, for some given $f$) or
{\em differential approximants} (with $t_i =d^if/dx^i$, for some given
$f$); see for instance~\cite[Chapitre~7]{BoChGiLeLeSaSc17}.

Expressed in the canonical monomial bases, the matrix of a
Hermite-Pad\'e problem has size $\sigma \times (n_0 + \cdots +
n_{s-1})$, and consists of $s$ lower triangular Toeplitz blocks. More
generally, our goal in this paper is to compute efficiently elements
in the kernel of {\em mosaic Toeplitz} matrices~\cite{HeAm88}, that
is, $m \times n$ matrices $\mT=(\mT_{i,j})_{1 \le i \le p,1 \le j \le
  q}$ with a $p \times q$ block structure, each of the $pq$ blocks $\mT_{i,j}$
being Toeplitz.

An $m \times n$ Toeplitz matrix
$\mT=(t_{i-j})_{1\le i \le m, 1 \le j \le n}$ can be succinctly
represented by the polynomial
$P_\mT=t_{-n+1} + t_{-n+2} x + \cdots + t_{m-1} x^{m+n-2}$;
multiplication of $\mT$ by a vector $\vb=[b_0~\cdots~b_{n-1}]^t$
amounts to computing $P_\mT P_\vb \bmod x^{m+n-1}$ and keeping the
coefficients of degrees $n-1,\dots,m+n-2$.  More generally, a mosaic
Toeplitz $\mT=(\mT_{i,j})_{1 \le i \le p,1 \le j \le q}$ can be
described by a sequence of $pq$ such polynomials
$\P=(P_{\mT_{i,j}})_{1 \le i \le p,1 \le j \le q}$, together
with the sequences $I=(m_1,\dots,m_p)$ and $J=(n_1,\dots,n_q)$ giving
the row-sizes, resp.\ column-sizes of the blocks. Then, our main
problem is the following.

\begin{pbm}\label{pb:mosaic}
  Given polynomials $\P$ and integers $I,J$ as above, 
  defining a mosaic Toeplitz matrix $\mT$, find a non-zero
  vector in the kernel of $\mT$.
\end{pbm}
We consider two situations, first over an arbitrary field $\K$,
counting all operations in $\K$ at unit cost (this is a good model for
computations over finite fields), then specifically over $\Q$, taking
bit-size into account. In all this paper, we closely follow the
existing formalism of {\em structured matrix computations}; the
algorithms in this paper are following previous work by
Morf~\cite{Morf80}, Bitmead-Anderson~\cite{BiAn80}, Kailath and
co-authors~\cite{KaKuMo79,KaSa99}, Pan~\cite{Pan90,Pan92},
Kaltofen~\cite{Kaltofen94}, Cardinal~\cite{Cardinal99}, etc.  We
complement the existing literature as follows:
\begin{itemize}
\item we describe an improved ``fast'' algorithm (of quadratic
  complexity with respect to the matrix size), that makes use of fast
  matrix multiplication;
\item we define a class of ``Cauchy-like'' matrices (see definitions
  below) for which the matrix-vector product is faster (by a constant
  factor) than previous designs;
\item we show how Pan's technique of ``multiplicative transformation''
  of operators outputs matrices that have generic rank profile (with
  high probability), so that further regularization is not needed in
  general;
\item for matrices defined over $\Q$, we introduce a
  divide-and-conquer algorithm as an alternative to Newton iteration,
  and we show how this algorithm can be improved in the case of
  algebraic approximation.
\end{itemize}
Another contribution of this paper is a discussion of the design and
practical performance of a C++ implementation of these algorithms.  To
our knowledge, only a few papers address these methods from the
practical viewpoint. An early reference is~\cite{SeShSp82}, which
concludes that the divide-and-conquer (MBA) algorithm for solving
Toeplitz matrices in quasi-linear time would require matrices of size
$10^6$ to break even with quadratic-time algorithms; a more recent
article~\cite{Huckle94} estimates the crossover point to be around
$8000$. 

Our experiments first consider computations over finite fields.  We
assess the practical impact of fast matrix multiplication for
structured matrix algorithms (such as in the new algorithm mentioned
above, or those in~\cite{BoJeSc08,BoJeMoSc16}), and we estimate for
what matrix size quasi-linear algorithms become effective (with much
lower crossover points than above). In the context of computations
over $\Q$, we show that our divide-and-conquer algorithm outperforms
Newton iteration consistently, and we demonstrate significant
speed-ups for the case of algebraic approximants.

This paper is organized as follows. We start in
Section~\ref{sec:basics} with a review of basic results on structured
matrices, including a discussion of transformation of operators and
regularization. Section~\ref{sec:abstract} describes algorithms
applicable in a unit cost model, with in particular a new algorithm
that uses fast matrix multiplication, and a discussion of the
practical performance of these algorithms. Finally,
Section~\ref{sec:lifting} presents lifting algorithms for structured
matrices, and the corresponding implementation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Basic results}\label{sec:basics}

This section reviews background material on structured matrices; for a
more comprehensive treatment, we refer the reader to~\cite{Pan01}.
In this section, $\K$ denotes our base field.

\smallskip\noindent{\bf \small Overview.}  Developed
in~\cite{KaKuMo79}, the {\it displacement operator} approach
associates to a matrix $\mA$ its displacement $\nabla(\mA)$, that is, the
image of $\mA$ under a \textit{displacement operator}~$\nabla$.  Then,
we say that $\mA$ is structured with respect to $\nabla$ if $\nabla(\mA)$
has a small rank compared to its size; the rank of $\nabla(\mA)$ is
called the \textit{displacement rank} of $\mA$ with respect to
$\nabla$. A prominent example is the family of so-called {\em
  Toeplitz-like} matrices, which are structured for the Toeplitz
displacement operator
\begin{eqnarray*}
  \phi: \mA & \mapsto & \left( \mZ_m \mA - \mA \mZ_n \right) = 
                        (\mA \downarrow) - (\mA \leftarrow)
\end{eqnarray*}
where the lower shift matrix $\mZ_n \in \K^{n \times n}$ is the matrix
with ones below the diagonal.  The displacement rank of a Toeplitz
matrix for this operator is at most two; the displacement rank of a
mosaic Toeplitz with a $p \times q$ block structure is at most $p+q$.

The key idea of most algorithms for structured matrices is summarized
by Pan's motto~\cite{Pan01}: compress, operate, decompress. Indeed,
for $\mA$ of size $m \times n$ over $\K$, if $\nabla(\mA)$ has
rank~$\alpha$, it can be represented using few elements through {\it
  $\nabla$-generators}, that is, two matrices $(\mG,\mH)$ in
$\K^{m\times \alpha} \times \K^{n\times \alpha}$, with $\nabla(\mA) =
\mG \mH^t$; $\alpha$ is the {\em length} of the generators. The main
idea behind algorithms for structured matrices is to use generators as
a compact data structure, involving $\alpha (m+n)$ field elements
instead of $mn$.

\smallskip\noindent{\bf \small Cauchy-like matrices.}  Beyond the
Toeplitz structure (and the directly related Hankel one), two other
important cases are the so-called Vandermonde and Cauchy
structures. While the case of Toeplitz-like matrices was the first one
to be studied in detail, we will actually focus on Cauchy-like
matrices, as we will see that this particular structure is quite
convenient to work with.

For a sequence $\vu=(u_1,\dots,u_m)$ in $\K^m$, let $\mD_\vu \in
\K^{m\times m}$ be the diagonal matrix with diagonal entries
$u_1,\dots,u_m$. Then, given $\vu$ as above and $\vv$ in $\K^n$, we will
consider the operator $\nabla_{\vu,\vv}: \mA \in \K^{m\times n} \mapsto \mD_\vu
\mA - \mA \mD_\vv$; {\em Cauchy-like} matrices (with respect to the
choice of $\vu$ and $\vv$) are those matrices $\mA$ for which
$\nabla_{\vu,\vv}(\mA)$ has small rank.

Let $\vu,\vv$ be given and suppose that $u_i \ne v_j$ holds for all
$i,j$. Then, the operator $\nabla_{\vu,\vv}$ is invertible: given
$\nabla_{\vu,\vv}$-generators $(\mG,\mH)$ of length $\alpha$ for $\mA$, we can reconstruct $\mA$ as
\begin{equation}\label{eq:recA}
\mA = \sum_{i=1}^\alpha
\mD_{\vg_i} 
\mC_{\vu,\vv}\,\mD_{\vh_i},\ \ \ 
\mC_{\vu,\vv}=\begin{bmatrix}
\frac 1{u_1-v_1} & \cdots & \frac 1{u_1-v_n}\\
\vdots & & \vdots \\
\frac 1{u_m-v_1} & \cdots & \frac 1{u_m-v_n}
\end{bmatrix}, 
\end{equation}
where $\vg_i$ and $\vh_i$ are the $i$th columns of respectively $\mG$
and~$\mH$, and  matrix $\mC_{\vu,\vv}$ is known as a {\em Cauchy
  matrix}. Remark that we can
equivalently rewrite $\mA$ as
\begin{equation}\label{eq:recA2}
\mA= (\mG \mH^t) \odot \mC_{\vu,\vv},
\end{equation}
where $\odot$ denotes the entrywise product.

We will have to handle submatrices of $\mA$ through their
generators. The fact that $\mD_{\vu}$ and $\mD_{\vv}$ are diagonal
matrices makes this easy (this is one of the aspects in which the
Cauchy structure behaves more simply than the Toeplitz one).  Suppose
that $(\mG,\mH)$ are generators for $\mA$, with respect to the
operator $\nabla_{\vu,\vv}$, and let $\vu_I=(u_i)_{i \in I}$ and
$\vv_J=(v_j)_{j \in J}$ be subsequences of respectively $\vu$ and
$\vv$, corresponding to entries of indices $I$ and $J$. Let
$\mA_{I,J}$ be the submatrix of $\mA$ obtained by keeping rows and
columns of indices respectively in $I$ and $J$, and let
$(\mG_I,\mH_J)$ be the matrices obtained from $(\mG,\mH)$ by
respectively keeping rows of $\mG$ of indices in $I$, and rows of
$\mH$ of indices in $J$. Then, $(\mG_I,\mH_J)$ is a
$\nabla_{\vu_I,\vv_J}$-generator for $\mA_{I,J}$.

Another useful property relates to inverses of Cauchy-like matrices.
If a matrix $\mA \in \K^{n\times n}$ is invertible, and is structured
with respect to an operator $\nabla_{\vu,\vv}$, its inverse is
structured with respect to $\nabla_{\vv,\vu}$: if $\mD_\vu \mA - \mA
\mD_\vv = \mG \mH^t$, one easily deduces that $\mD_\vv \mA^{-1} -
\mA^{-1} \mD_\vu = - (\mA^{-1} \mG) (\mA^{-t} \mH)^t$, where $\mA^{-t}$
is a shorthand for $(\mA^{-1})^t$.


\smallskip\noindent{\bf \small Algorithms.}  In most the paper, we
give costs in an algebraic model, counting base field operations at
unit cost (in Section~\ref{sec:lifting}, we work over $\Q$ and use 
a boolean model).

We let $\M$ be such that over any ring, polynomials of degree at most
$d$ can be multiplied in $\M(d)$ base ring operations; we also assume
that the super-linearity assumptions of~\cite[Chapter~8]{GaGe13}
hold. Using the Cantor-Kaltofen algorithm~\cite{CaKa91}, we can take
$\M(d)\in O(d \log(d)\log\log(d))$. We let $\omega$ be a feasible
exponent for linear algebra, in the sense that matrices of size $n$
can be multiplied in $O(n^\omega)$ base ring operations over any ring;
the best bound to date is $\omega < 2.38$~\cite{CoWi90, LeGall14}.
The notation $\Otilde{\,}$ indicates that we omit polylogarithmic
terms. Among the linear algebra operations whose cost reduces to matrix
multiplication, we will have to compute rank and rank
profile~\cite{IbMoHu82}.

Matrix-vector multiplication with $\mC_{\vu,\vv}$ reduces to degree
$n$ polynomial interpolation at the points $\vv$ and evaluation at the
points $\vu$. Using fast polynomial evaluation and interpolation, this
can be done in time $O(\M(\mx)\log(\mx))$, with $\mx=\max(m,n)$; thus, we
can multiply matrix $\mA$ of~\eqref{eq:recA} by a vector in time
$O(\alpha \M(\mx)\log(\mx))\subset \Otilde{\alpha \mx}$.

In~\cite[Theorem~4.7.3]{Pan01}, Pan shows that if the entries of both
$\vu$ and $\vv$ are in {\em geometric progression}, one can reduce the
cost of the matrix-vector multiplication by $\mC_{\vu,\vv}$ to
$O(\M(\mx))$, since polynomial evaluation or interpolation at $n$
points in geometric progression can be done in time $O(\M(n))$;
similarly, multiplication by $\mA$ as above takes time
$O(\alpha\M(\mx))$.

\begin{remark}\label{rmk:factor3}
We propose here a refinement of this idea, that allows us to
save a constant factor in runtime: we require that $\vu$ and $\vv$ be
geometric progressions with {\em the same ratio} $\tau$. Then, the
cauchy matrix $\mC_{\vu,\vv}$ has entries $1/(u_i -
v_j) = 1/(u_1 \tau^{i-1} - v_1 \tau^{j-1})$, so it can be factored as
$$\mC_{\vu,\vv}=\mD_{\tau^{-1}}
\begin{bmatrix}
\frac 1{u_1 - v_1} & \frac 1{u_1 - v_1 \tau} & \cdots & \frac 1{u_1-v_1 \tau^{n-1}}\\
\frac 1{u_1 - v_1 \tau^{-1}}  & \frac 1{u_1 - v_1} & \cdots & \frac 1{u_1-v_1 \tau^{n-2}}\\
\vdots & & \vdots \\
\frac 1{u_1 - v_1 \tau^{1-m}}  & \frac 1{u_1 - v_1 \tau^{2-m}} & \cdots & \frac 1{u_1-v_1 \tau^{n-m}}
\end{bmatrix},$$
where $\mD_{\tau^{-1}}$ is diagonal with entries
$(1,\tau^{-1},\tau^{-2},\dots,\tau^{1-m})$, and where the right-hand matrix
is Toeplitz. In the reconstruction formula~\eqref{eq:recA}, the
diagonal matrix $\mD_\tau$ commutes with all matrices $\mD_{g_i}$, so
we can take it out of the sum. Hence, we replaced $\alpha$ evaluations
/ interpolations at geometric progressions by $\alpha$ product by
Toeplitz matrices, each of which can be done in a single polynomial
multiplication. The cost for a matrix-vector product by $\mA$ remains
$O(\alpha \M(\mx))$, but the constant in the big-O is lower: for $m=n$,
using middle product techniques~\cite{HaQuZi04,BoLeSc03,BoSc05}, the
cost goes down from $3\alpha \M(\mx) +O(\alpha \mx)$ to $\alpha \M(\mx)
+O(\alpha \mx)$. 
\end{remark}

If one needs to multiply $\mA$ as above by {\em several} vectors,
further improvements are possible: we mention without giving details
an algorithm from~\cite{BoJeMoSc16}, that itself
follows~\cite{BoJeSc08}, and which makes it possible to multiply $\mA$
by $\alpha$ vectors in time $O(\alpha^{\omega-1} \M(\mx))$ instead of
$O(\alpha^2 \M(\mx))$, by reduction to a sequence of polynomial matrix
multiplications.

\smallskip\noindent{\bf \small Reduction from mosaic Toeplitz to Cauchy structure.}
We said above that our primary interest lies in mosaic
Toep\-litz matrices. An important insight of Pan~\cite{Pan90} shows
that one can reduce questions about Toeplitz-like matrices to ones
about Cauchy-like matrices (and conversely, if one wishes to), for a
moderate cost overhead. In this paragraph, we give details on 
this transformation, following~\cite[Chapter~4.8]{Pan01}.

To a vector $\vu$ in $\K^m$, let us further associate the
corresponding Vandermonde matrices $\mV_\vu, \mW_\vu \in \K^{m \times
  m}$
$$
\mV_\vu = 
\begin{bmatrix}
  1 & u_1 & \cdots & u_1^{m - 1}\\
  \vdots & \vdots &  & \vdots\\
  1 & u_m & \cdots & u_m^{m - 1}
\end{bmatrix},
\mW_\vu = 
\begin{bmatrix}
  u_1^{m - 1} & \cdots & u_m^{m - 1}\\
  \vdots &  & \vdots\\
  u_1 & \cdots & u_m\\
  1 & \cdots & 1
\end{bmatrix}.
$$
For $\vu$ as above, $\vv$ in $\K^n$ and $\mT$ in $\K^{m\times n}$, we
let $\mA = \mV_\vu\, \mT\, \mW_\vv$; we now prove that if $\mT$ is Toeplitz-like,
 $\mA$ is Cauchy-like. 

The $\nabla_{\vu,\vv}$-generators of $\mA$ depend on the
$\phi$-generators of $\mT$, so let us start with those. If
$\mT=(\mT_{i,j})_{1 \le i \le p,1 \le j \le q}$ is mosaic Toeplitz of
size $m \times n$, with $\mT_{i,j}$ of size $m_i \times n_j$, we
define the following.
\begin{itemize}
\item For $1 \le i \le p$, let $\mT_{i,*}$ be the block matrix
  $[\mT_{i,1}~\cdots~\mT_{i,q}]$.  Let $\eta_i$ be the first row of
  $\mT_{i,*}$, shifted left once, let $\eta'_i$ be the last row of
  $\mT_{i,*}$, and finally let $\vh_i=(\eta'_{i-1}-\eta_i)$ (for
  $i=1$, $\eta'_0$ is the zero vector); this is a row-vector of size
  $n$. If $\ve_{i,n}$ denotes the row-vector of dimension $n$ with
  only a one at index $i$, we also let $\vg_{i}$ be the transpose of
  $\ve_{m_1 + \cdots +m_{i-1}+1,m}$.
  
\item For $1 \le j \le q$, define $\mT_{*,j}$ similarly to
  $\mT_{i,*}$; $\gamma_j$ and $\gamma'_j$ are its first and last
  columns, with now a downward shift for $\gamma'_j$; in addition,
  their entry of index $m_1 + \cdots + m_j$ are set to zero. Then, we
  define $\vg_{p+j}=(\gamma'_{j}-\gamma_{j+1})$ ($\gamma_{q+1}$ is
  set to zero).  We also let $\vh_{p+j}=\ve_{n_1 + \cdots +n_j,n}$.
\end{itemize}
With these definitions, the matrices
$\mG=\left< \vg_1 | \dots | \vg_{p+q} \right>$ and
$\mH= \left<\vh^t_1 | \dots | \vh^t_{p+q} \right>$ are $\phi$-generators of
$\mT$.

To obtain $\mA$, we conjugate $\mT$ by Vandermonde matrices, since $
\mD_{\vu} \mV_{\vu} - \mV_{\vu} \mZ_m$ and $ \mW_{\vv} \mD_{\vv} -
\mZ_n \mW_{\vv}$ have small rank; explicitly, they are equal to
respectively $(\vu^m)^t \, \ve_{m,m}$ and $(\ve_{1,n})^t \, \vv^n$,
where $\vu^m = \begin{bmatrix} u_1^m & \cdots & u_n^m \end{bmatrix}$,
and similarly for $\vv^n$. As a
consequence, since $\phi(\mT)=\mG \mH^t$, we have
\begin{eqnarray}
  \nabla_{\vu, \vv} \left( \mA \right) & = & \mD_{\vu}  \mV_{\vu} 
  \mT  \mW_{\vv} - \mV_{\vu}  \mT  \mW_{\vv}  \mD_{\vv} \nonumber \\
  & = & \mV_{\vu}  \left( \mZ  \mT - \mT  \mZ \right) \mW_{\vv} 
  \label{eq:gentoetogencau}\\
  &   & + (\vu^m)^t \, \ve_{m,m}  \mT  \mW_{\vv} 
        - \mV_{\vu}  \mT  (\ve_{1,n})^t \, \vv^n . \nonumber 
\end{eqnarray}
This motivates the following definitions.
\begin{itemize}
\item For $1 \le i \le p$, let $\vh'_i=\vh_i \mW_\vv$ and also let $\vg'_{i}$ be
  the column of $\mV_\vu$ of index $m_1 + \cdots +m_{i-1}+1$.
  
\item For $1 \le j \le q$, define $\vg'_{p+j} = \mV_\vu \vg_{p+j}$ and also let
  $\vh'_{p+j}$ be the row of index $n_1 + \cdots +n_j$ in $\mW_\vv$.

\item We define $\vg_{p+q+1} = (\vu^m)^t$, and $\vh_{p+q+1}$ as the last row of
  $\mT \mW_\vv$; we also define $\vg_{p+q+2}$ as the first column of
  $-\mV_\vu \mT$, and $\vh_{p+q+2} = \vv^n$.
\end{itemize}
%
It follows from Eq.~(\ref{eq:gentoetogencau}) that
$\mG'= \left< \vg'_1 | \dots | \vg'_{p+q+2}\right>$, and $\mH'$ with
columns that are the transpose of $\vh'_1,\dots,\vh'_{p+q+2}$, are
$\nabla_{\vu,\vv}$-generators of $\mA$ of length $p+q+2$ (whereas
$\mT$ has $\phi$-generators of length $p+q$). The bottleneck in the
computation of $\mG'$ and $\mH'$ are $p$ left-products by $\mW_\vv$
and $q$ products by $\mV_\vu$.  If all entries of $\vu$ and $\vv$ are
in geometric progression this takes time $O(p \M(n) + q \M(m))$.

\smallskip\noindent{\bf \small Regularization.}  In our algorithms for
Cauchy-like matrices, we will assume that the input matrix has {\em
  generic rank profile}, that is, that its leading principal minors of
size up to its rank are invertible.  Regularization for structured
matrices was introduced for this purpose by
Kaltofen~\cite{Kaltofen94}, for the Toeplitz structure. In our Cauchy
context, one could apply to $\mA$ as defined above the regularization
procedure from~\cite[Section~5.6]{Pan01}, which consists in replacing
$\mA$ by $\mA' = \mD_\vx \mC_{\va,\vu} \mA \mC_{\vv,\vb}\mD_\vy$,
for some new vectors $\vx,\va \in \K^m$ and $\vy,\vb \in
\K^n$. Theorem~5.6.2 in~\cite{Pan01} shows that if $\va,\vu$ consist
of $2m$ distinct scalars, and $\vb,\vv$ consist of $2n$ distinct
scalars, then there exists a non-zero polynomial $\Delta$ in the
entries of $\vx$ and $\vy$, of degree at most $\mn=\min(m,n)$ in each
block of variables, such that the non-vanishing of $\Delta$ implies
that $\mA'$ has generic rank profile (that theorem is stated for
square matrices, but the result holds in the rectangular case as
well).  The downside of this contruction is that it requires to
compute a pair of generators of length $p+q+4$ for $\mA'$, involving
the multiplication of $\mG'$ and $\mH'$ by $\mC_{\va,\vu}$ and
$\mC_{\vv,\vb}$.

\smallskip

We now point out a simpler property, involving only  $\mA = \mV_\vu\, \mT\,
\mW_\vv$ as above. For the rest of this paragraph, we assume that the
entries of $\vu$ and $\vv$ are indeterminates over $\K$, and we show
that $\mA$ has generic rank profile; this will imply that the same
property holds for a generic choice of $\vu,\vv$ with entries in
$\K$. This construction is clearly favorable over the one above, since
it involves no extra computation on $\mA$.

Following the proof of~\cite[Theorem~5.6.2]{Pan01}, we can use the
Cauchy-Binet formula to express the minors of $\mA$ in terms of those
of $\mV_\vu$, $\mT$ and $\mW_\vv$ as follows. Let $i \le {\rm
  rank}(\mT)$ and let $I=\{1,\dots,i\}$. The determinant $\delta_i$ of
the $i$th leading principal minor of $\mA$ is the sum over all
$J=\{j_1,\dots,j_i\} \subset \{1,\dots,m\}$ and $K=\{k_1,\dots,k_i\}
\subset \{1,\dots,n\}$ of the terms $\alpha_{I,J}\, \beta_{J,K}\,
\gamma_{K,I}$, where $\alpha_{I,J}$ is the determinant of
$(\mV_\vu)_{I,J}$, $\beta_{J,K}$ is the determinant of $\mT_{J,K}$,
and $\gamma_{K,I}$ is the determinant of $(\mW_\vv)_{K,I}$.

Let $<$ be the monomial ordering on the variables $u_1,\dots,u_i$,
$v_1,\dots,v_i$ that first sorts monomials using the lexicographic order on
$u_1,\dots,u_i$ with $u_1 < \dots < u_i$, and then breaks the ties using the
lexicographic order on $v_1,\dots,v_i$ with $v_1 > \dots > v_i$. If
$J=\{j_1,\dots,j_i\}$ with $j_1 < \dots < j_i$ then the leading monomial
(denoted $\lm(\cdot)$) of $\alpha_{I,J}$ is
$\vu^J := u_1^{j_1-1} \cdots u_i^{j_i-1}$.  Similarly if $K=\{k_1,\dots,k_i\}$
with $k_1 < \cdots < k_i$, then
$\vv^K := v_1^{n-k_1} \cdots v_i^{n-k_i}=\lm(\gamma_{K,I})$. Since $\mT$ is of
rank greater of equal to $i$, at least one of its $i$th minor $\beta_{J,K}$ does
not vanish. Let $J_{\max},K_{\max}$ be the pair of subsets that maximizes
$\vu^J \vv^K$ amongst those for which $\beta_{J,K} \neq 0$. Then we must have
$\lm(\det(\mA_{I,I})) = \vu^{J_{\max}} \vv^{K_{\max}}$, which shows that
$\det(\mA_{I,I})$ is non-zero. Note that the partial degrees in all variables
of $\det(\mA_{I,I})$ is at most $\max(n,m)$. 

\todo{See if we can derive anything for geometric sequences $\vu, \vv$
  with the same ratio}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Over an abstract field}\label{sec:abstract}

In this section, we work over a field $\K$, and we explain how to
solve Problem~\ref{pb:mosaic} by using Pan's reduction to the
Cauchy-like Problem~\ref{pb:cauchy} below. Even though our goal is to
solve a linear system, the algorithms do slightly more: they compute
the inverse of a given matrix (or of a maximal minor thereof); this is
similar to what happens for dense matrices, where it is not known how
to solve linear systems with an exponent better than $\omega$.

The main question of this section is the following; remark that the
assumptions on the vectors $\vu,\vv$ are slightly stronger than the
one required for $\nabla_{\vu,\vv}$ to be invertible.
\begin{pbm}\label{pb:cauchy}
  Consider vectors $\vu=(u_1,\dots,u_m)$ and $\vv=(v_1,\dots,v_n)$,
  such that $(\vu,\vv)$ has $m+n$ distinct entries. Given
  $\nabla_{\vu,\vv}$-generators of length $\alpha$ for a matrix $\mA$
  in $\K^{m \times n}$, with $\alpha \le \min(m,n)$, do the following:
  \begin{itemize}
  \item if $\mA$ does not have generic rank profile, raise an error,
  \item else, return $\nabla_{\vv',\vu'}$-generators for the inverse
    of the leading principal minor of $\mA$ of order $r$, with
    $\vv'=(v_1,\dots,v_r)$ and $\vu'=(u_1,\dots,u_r)$, where
    $r={\rm rank}(\mA)$.
\end{itemize}
\end{pbm}
There exist two major classes of algorithms for handling such
problems, iterative ones, of cost that grows like $mn$ (for fixed
$\alpha$), and algorithms using divide-and-conquer techniques, of
quasi-linear cost in $m+n$. We stress that having a fast
quadratic-time algorithm is actually crucial in practice: as is the
case for the Half-GCD, fast linear algebra algorithms, etc, the
quasi-linear algorithm will fall back on the quadratic one for input
sizes under a certain threshold, and the performance of the latter
will be an important factor in the overall runtime. 

Iterative algorithms that solve a size $n$ Toeplitz system in time
$O(n^2)$ have been known for
decades~\cite{Levinson47,Durbin60,Trench64}; extensions to structured
matrices were later given, as for instance in~\cite{KaGoOl95}. After a
section of preliminary results, we give in Section~\ref{ssec:quad} an
algorithm inspired by~\cite[Algorithme~4]{Mouilleron08}, for the
specific form of our Problem~\ref{pb:cauchy}. In this reference,
Mouilleron sketches an algorithm that solves Problem~\ref{pb:cauchy}
in time $O(\alpha n^2)$, in the case where $m=n$ and $\mA$ is
invertible (but without the rank profile assumption); he credits the
origin of this algorithm to Kailath~\cite[\S1.10]{KaSa99}, who dealt
with symmetric matrices.  Our algorithm follows the same pattern, but
reduces the cost to $O(\alpha^{\omega-2} n^2)$.

In Section~\ref{ssec:MBA-C}, we review divide-and-conquer
techniques. Kaltofen~\cite{Kaltofen94} gave a divide-and-conquer
algorithm that solves the analogue of Problem~\ref{pb:cauchy} for
Toeplitz-like matrices, lifting assumptions of (strong)
non-singularity needed in the original Morf and Bitmead-Anderson
algorithm~\cite{Morf80,BiAn80}; a generalization to the Cauchy case is
in~\cite{PaZh00,Cardinal99}.  A further improvement due to Jeannerod
and Mouilleron~\cite{JeMo10}, following~\cite{Cardinal99}, allows one
to bypass costly compression stages that are needed in Kaltofen's
algorithm and its extensions, by predicting the shape of the
generators we have to compute. For the case of square Cauchy-like
matrices of size $n$, this results in an algorithm of cost
$O(\alpha^2 \M(n)\log(n)^2)$. As pointed out
in~\cite[Theorem~5.3.1]{Pan01} (see also~\cite{ChPa00}), the cost can
be reduced to $O(\alpha^2 \M(n)\log(n))$ by choosing vectors $\vu$ and
$\vv$ in geometric progression. In our case, we will use vectors
$\vu$, $\vv$ that are in geometric progression with the same ratio, in
order to apply Remark~\ref{rmk:factor3}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Cauchy generators of the inverse}\label{ssec:genofinv}

Let $(\mG,\mH) \in \K^{m\times \alpha} \times \K^{n\times \alpha}$ be
$\nabla_{\vu,\vv}$-generators of a matrix $\mA$ with respect to the
operator $\nabla_{\vu,\vv}$, with $\vu=(u_1,\dots,u_m)$ and
$\vv=(v_1,\dots,v_n)$. Let further $r$ be the rank of $\mA$. Our goal
is to decide if $\mA$ has generic rank profile, and if so, to return
generators $(\mY,\mZ) \in \K^{r\times \alpha} \times \K^{r\times
  \alpha}$ of the inverse of the leading principal minor of $\mA$.


Let $\mn=\min(m,n)$. For $0\leq i \leq \mn$, write
$
\mA=\begin{bmatrix}
    \mA^{(i)}_{0,0} & \mA^{(i)}_{0,1} \\[1mm]
    \mA^{(i)}_{1,0} & \mA^{(i)}_{1,1}
  \end{bmatrix}\!\!.
$
If ${\mA^{(i)}_{0,0}}$ is invertible, we define, similarly to~\cite{Cardinal99},
$$
\mS^{(i)} = 
\begin{bmatrix} 
     \mS^{(i)}_{0,0} & \mS^{(i)}_{0,1}\\
     \mS^{(i)}_{1,0} & \mS^{(i)}_{1,1}\\
\end{bmatrix} = 
\begin{bmatrix} 
  \mA^{(i)}_{1,1} - \mA^{(i)}_{1,0} {\mA^{(i)}_{0,0}}{}^{-1} \mA^{(i)}_{0,1} 
  & 
  \mA^{(i)}_{1,0} {\mA^{(i)}_{0,0}}{}^{-1} 
  \\[1mm]
  -{\mA^{(i)}_{0,0}}{}^{-1} \mA^{(i)}_{0,1}
  &  
  {\mA^{(i)}_{0,0}}{}^{-1} 
\end{bmatrix}.
$$
(the order of block rows and columns we use differs
from that reference.) We see that $( \mS^{(i)})_{i=0\dots \mn}$ form a sequence of
matrices starting from $\mA=\mS^{(0)}$ and ending at
$\mA^{-1}=\mS^{(\mn)}$, at least when $\mA$ is square and invertible. To
understand where these matrices come from, we introduce the matrix
$ \mR^{(0)} = \begin{bmatrix}
  \mA \\
  \mI_{\mn, n}
\end{bmatrix} 
$
in $\K^{(m + \mn) \times n} $, where $\mI_{\mn, n} \in \K^{\mn \times n}$ is
the rectangular matrix with ones on the main diagonal. If one applies to
$\mR^{(0)}$ the column operations that reduce its first $i$th rows
$\begin{bmatrix} \mA^{(i)}_{0,0} & \mA^{(i)}_{0,1} \end{bmatrix}$ to
$\begin{bmatrix} 0 & \mI_{i,i} \end{bmatrix}$ using $\mA^{(i)}_{0,0}$
as pivot, we get
\begin{equation}\label{eq:Ri}
\mR^{(i)} =
\begin{bmatrix}
  \mS^{(i)} \\
  \mI_{\mu, n}
\end{bmatrix} 
= \mP^i \ \mR^{(0)} 
\begin{bmatrix}
  - \mA_{0, 0}^{(i)} \phantom{}^{- 1} \mA_{0, 1}^{(i)}
  & 
  \mA_{0, 0}^{(i)} \phantom{}^{- 1} \\
 \mI_{n - i,n-i} & 0
\end{bmatrix}
\end{equation}
where $\mP$ is a cyclic permutation matrix than moves the rows up by one place.

Given integers $a,b$, $\vu_{a:b}$ denotes the sequence
$(u_a,\dots,u_b)$ (and similarly for $\vv_{a:b}$); we then define
$\vu^{(i)}=(\vu_{i+1:m},\vv_{1:i})$ and
$\vv^{(i)}=(\vv_{i+1:n},\vu_{1:i})$.
A key result for the sequel is Lemma~\ref{lemma:cpj-cm}, which
is~\cite[Proposition~1]{Cardinal99}; it gives formulae for
$\nabla_{\vu^{(i)},\vv^{(i)}}$-generators of $\mS^{(i)}$ (remark that
this operator is invertible, in view of our assumption on $\vu$ and
$\vv$).  Define the matrices $(\mY^{(i)},
\mZ^{(i)})$ of $\mS^{(i)}$ by decomposing $\mG,\mH$ as
\[
 \mG=\begin{bmatrix} 
   \mG^{(i)}_0 \\ \mG^{(i)}_1
  \end{bmatrix},\ 
  \mH=\begin{bmatrix} 
        \mH^{(i)}_0 \\    \mH^{(i)}_1 
  \end{bmatrix}
\]
with $\mG^{(i)}_0, \mH^{(i)}_0 \in \K^{i \times \alpha}$ and defining
$(\mY^{(0)},\mZ^{(0)})=(\mG,\mH)$ and 
\[
\!\!\mY^{(i)}\!=\! 
\begin{bmatrix}
-\mA^{(i)}_{1,0}{\mA^{(i)}_{0,0}}{}^{-1}\mG^{(i)}_0 + \mG^{(i)}_1 \\
-{\mA^{(i)}_{0,0}}{}^{-1} \mG^{(i)}_0 
\end{bmatrix}\!,
\mZ^{(i)}\!=\! 
\begin{bmatrix}
-{\mA^{(i)}_{0,1}}{}^t{\mA^{(i)}_{0,0}}{}^{-t}\mH^{(i)}_0 + \mH^{(i)}_1 \\
{\mA^{(i)}_{0,0}}{}^{-t} \mH^{(i)}_0 
\end{bmatrix}.
\]
\begin{lemma}\label{lemma:cpj-cm}
 $(\mY^{(i)},\mZ^{(i)})$ are $\nabla_{\vu^{(i)},\vv^{(i)}}$-generators for $\mS^{(i)}$.
\end{lemma}
If $\mA$ has generic rank profile, then Lemma~\ref{lemma:cpj-cm} shows
that for $r=\rank(\mA)$, $(\mY^{(r)}_0,\mZ^{(r)}_0)$ are
$\nabla_{\vv',\vu'}$-generators for ${\mA^{(r)}_{0,0}}{}^{-1}$, for
$\vu',\vv'$ as in Problem~\ref{pb:cauchy}, so they solve our problem.
It remains to explain how to compute these matrices.

Let $i,j$ be non-negative integers with $0 \le i+j \le \mn$, and
${\mA^{(i)}_{0,0}}$ invertible. Decompose $\mS^{(i)}$, $\mY^{(i)}$ and
$\mZ^{(i)}$ into blocks
\begin{equation*}
\mS^{(i)} = \begin{bmatrix} 
\mS^{(i,j)}_{0,0} & \mS^{(i,j)}_{0,1} \\
\mS^{(i,j)}_{1,0} & \mS^{(i,j)}_{1,1}
\end{bmatrix},  
\mY^{(i)} = 
\begin{bmatrix}
  \mY^{(i,j)}_0 \\\mY^{(i,j)}_1
\end{bmatrix},
\mZ^{(i)} = 
\begin{bmatrix}
  \mZ^{(i,j)}_0 \\\mZ^{(i,j)}_1
\end{bmatrix}
\end{equation*}
with $\mS^{(i,j)}_{0,0} \in \K^{j \times j}$ and
$\mY^{(i,j)}_0, \mZ^{(i,j)}_0 \in \K^{j \times \alpha}$. Also decompose 
%
$\vu^{(i)} = 
\big[ \vu^{(i,j)}_0 \  \vu^{(i,j)}_1 \big]
$ 
%
with $\vu^{(i,j)}_0 \in \K^j$ and the same for $\vv^{(i)}$.
Operations that derive $\mS^{(i+j)}$ from
$\mS^{(j)}$ and $\mS^{(i)}$ from $\mS^{(0)}$ are similar, and a direct
calculation shows that
%
\begin{equation}
\label{eq:Si+j}
\mS^{(i+j)} = 
\begin{bmatrix} 
  \mS^{(i,j)}_{1,1} - \mS^{(i,j)}_{1,0} {\mS^{(i,j)}_{0,0}}{}^{-1} \mS^{(i,j)}_{0,1} 
  & 
  \mS^{(i,j)}_{1,0} {\mS^{(i,j)}_{0,0}}{}^{-1} 
  \\[1mm]
  -{\mS^{(i,j)}_{0,0}}{}^{-1} \mS^{(i,j)}_{0,1}
  &  
  {\mS^{(i,j)}_{0,0}}{}^{-1} 
\end{bmatrix}
\end{equation}
which implies the following formulae on the generators
\begin{align}
\mY^{(i+j,m-j)}_1&= -{\mS^{(i,j)}_{0,0}}{}^{-1} \,\mY^{(i,j)}_0,\quad
\mZ^{(i+j,n-j)}_1= {\mS^{(i,j)}_{0,0}}^{-t} \,\mZ^{(i,j)}_0 \nonumber\\
\mY^{(i+j,m-j)}_0&=-\mS^{(i,j)}_{1,0} \,\mY^{(i+j,m-j)}_1 + \mY^{(i,j)}_1,\label{eq:Yi+j}\\
\mZ^{(i+j,n-j)}_0&=-{\mS^{(i,j)}_{0,1}}{}^t \,\mZ^{(i+j,n-j)}_1 + \mZ^{(i,j)}_1 \nonumber;
\end{align}
%
this generalizes previous formulae for $(\mY^{(i)},\mZ^{(i)})$.  The
intuition behind Eq.~(\ref{eq:Yi+j}) is that by decomposing the column
operations that reduce
$\begin{bmatrix} \mA^{(i+j)}_{0,0} & \mA^{(i+j)}_{0,1} \end{bmatrix}$
to $\begin{bmatrix} 0 & \mI_{i+j,i+j} \end{bmatrix}$ (see
Eq.~(\ref{eq:Ri})) into two operations that respectively reduce the
first $i$th rows using $\mA^{(i)}_{0,0}$ as pivot, then the next $j$th
rows using $\mS^{(i,j)}_{0,0}$ as pivot, we split the computation of
$\mR^{(i+j)}$ into two similar operations that map $\mR^{(0)}$ to
$\mR^{(i)}$ and $\mR^{(i)}$ to $\mR^{(i+j)}$.

Finally, to solve Problem~\ref{pb:cauchy}, we need to detect when $\mA$ has
generic rank profile. By construction, $\mS^{(i,j)}_{0,0}$ is the
Schur complement of ${\mA^{(i)}_{0,0}}$, seen as a submatrix of
$\mA^{(i+j)}_{0,0}$. This implies the following result.
%
\begin{lemma}\label{lemma:update}
  Assume that $\mA^{(i)}_{0,0}$ is invertible and has generic rank
  profile.
  $\mA^{(i+j)}_{0,0}$ has generic rank profile if and only if
  $\mS^{(i,j)}_{0,0}$ does, and
  ${\rm rank}(\mA^{(i+j)}_{0,0})=i+{\rm rank}(\mS^{(i,j)}_{0,0})$. 
\end{lemma}
%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{A faster iterative algorithm}\label{ssec:quad}

\begin{algorithm}[t]
  \LinesNumbered \DontPrintSemicolon
  \caption{Iterative algorithm $\Iter$ for Problem~\ref{pb:cauchy} \label{algo:iter}}

  \Input{Generators $(\mG,\mH)$ of $\mA$ and step size $\beta$}
  \Output{$r=\rank(\mA)$, generators of $\mA^{(r)}_{0,0}{}^{-1}$}
  % \SetKwFunction{Algorithm}{RDAC} \Algorithm{$A,\mathbf{C},i,N,k$}

  $i=0, \mY^{(0)}=\mG, \mZ^{(0)}=\mH, \bar{r} = \mn$\;
  \While{$i \neq \bar{r}$}{
    $j = \min (\beta,\bar{r}-i)$\;
    Recover $\mS^{(i,j)}_{0,0}$ from its generators $(\mY^{(i,j)}_0,\mZ^{(i,j)}_0)$\;
    Compute the rank $\rho$, rank profile, inverse of $\mS^{(i,j)}_{0,0}$\; \label{line:rankprof}
    \If{$\mS^{(i,j)}_{0,0}$ does not have generic rank profile}{
      \Raise\;
    }
    \lIf{$\rho<j$}{$j=\rho,\ \bar{r}=i+\rho$}
    Recover $\mS^{(i,j)}_{0,1}$, $\mS^{(i,j)}_{1,0}$ 
    from their generator\; \label{line:rowcol}

    Compute the generators $(\mY^{(i+j)},\mZ^{(i+j)})$ 
    of $\mS^{(i+j)}$\; \label{line:nextgen}

    $i = i + j$\;
  }
  Recover $\mS^{(\bar{r})}$ from its generators $(\mY^{(\bar{r})},\mZ^{(\bar{r})})$
  \label{line:recoverwhole}\;

  Read in $\mS^{(\bar{r})}$ the Schur complement of $\mA^{(\bar{r})}_{0,0}$ in $\mA$\;
  \lIf{the Schur complement is non zero}{\Raise}
  \lElse {\Return $(\bar{r},\mY^{(\bar{r},m-\bar{r})}_1,\mZ^{(\bar{r},n-\bar{r})}_1)$}
\end{algorithm}

We describe in Algorithm~\ref{algo:iter} a quadratic-time iterative
algorithm for Problem~\ref{pb:cauchy}; we will use a step size
$\beta \in \{1,\dots,\alpha\}$, given as a parameter.  Note that the
formulae that computes $(\mY^{(i+j)},\mZ^{(i+j)})$ from
$(\mY^{(i)},\mZ^{(i)})$ only involves $\mS^{(i,j)}_{0,0}{}^{-1}$,
$\mS^{(i,j)}_{0,1}$, $\mS^{(i,j)}_{1,0}$. So
Algorithm~\ref{algo:iter}, line~\ref{line:rowcol} recovers
$\mS^{(i,j)}_{0,1}$ from its
$\nabla_{\vu^{(i,j)}_0, \vv^{(i,j)}_1}$-generators
$(\mY^{(i,j)}_0,\mZ^{(i,j)}_1)$ and the same for $\mS^{(i,j)}_{1,0}$.

\smallskip\noindent{\bf \small Termination and correction.}
The correction of the algorithm is based on the fact that $\mA$ has
generic rank profile if and only if
$\rank(\mA) = \max\{ r \ |\  \forall \ell \leq r, \det(\mA^{(\ell)}_{0,0}) \neq
0 \}$.

The while loop of the algorithm will find the maximal leading
principal minor of $\mA$ that is non-zero and has generic rank
profile. If at some step $i$, we know that $\mA^{(i)}_{0,0}$ is
invertible and has generic rank profile, we use
Lemma~\ref{lemma:update} to determine if $\mA^{(i+j)}_{0,0}$ has the
same properties.

The while loop exits on one of two conditions that both ensures that
$\bar{r} = \max\{ r \ |\ \forall j \leq r, \det(\mA^{(j)}_{0,0}) \neq
0 \}$.
Either $\rank(\mS^{(i,j)}_{0,0})$ is always $j$ (then $i$ will
eventually reach $\bar{r}=\mn$), or $\rank(\mS^{(i,j)}_{0,0})<j$ at some
point and after setting $\bar{r}=i+\rank( \mS^{(i,j)}_{0,0})$, we
have $\det(\mA^{(\ell)}_{0,0}) \neq 0$ for all $\ell \leq \bar{r}$
and $\det(\mA^{(\bar{r}+1)}_{0,0}) = 0$, and the while loop exits
right after.

After the while loop, it remains to test if $\rank(\mA) = \bar{r}$
which is done by checking that the Schur complement of
$\mA^{(\bar{r})}_{0,0}$ in $\mA$ is zero.

\smallskip\noindent{\bf \small Complexity.} Computing the rank profile
in line~\ref{line:rankprof} costs $O(\beta^\omega)$.  Using block
matrix multiplication in Eq.~\eqref{eq:recA2}, we can recover the
matrices of line~\ref{line:rowcol} in time
$O(\beta^{\omega-2} \alpha (m+n))$ since $j \le \beta \le \alpha$.  To
compute $\mY^{(i+j)}$ on line~\ref{line:nextgen}, we first compute
$\mS^{(i,j)}_{0,0}{}^{-1}$ in time $O(\beta^{\omega})$,
$\mS^{(i,j)}_{0,0}{}^{-1} \mY^{(i,j)}_1$ in time
$O(\beta^{\omega-1} \alpha)$; then all other operations take time
$O(\beta^{\omega-2} \alpha m)$.  Similarly, computing $\mZ^{(i+j)}$
takes time $O(\beta^{\omega-2} \alpha n)$.  Recovering the whole
matrix $\mS^{(\bar{r})}$ on line~\ref{line:recoverwhole} from its
$\nabla_{\vu^{(\bar{r})},\vv^{(\bar{r})}}$-generators
$(\mY^{(\bar{r})},\mZ^{(\bar{r})})$ is done by means
of~\eqref{eq:recA2} with a cost $O(\alpha^{\omega-2} mn)$ using block
matrix multiplication.

One iteration of the while loop costs
$O(\beta^{\omega-2} \alpha (m+n))$; we iterate $O(\mn/\beta)$ times, for
a total cost of $O(\beta^{\omega-3} \alpha \mn(m+n))$, which is
$O(\beta^{\omega-3} \alpha mn)$. This dominates the cost of
line~\ref{line:recoverwhole}, so that the whole running time is
$O(\beta^{\omega-3} \alpha mn)$. The algorithm of~\cite{Mouilleron08}
uses $\beta=1$, for which the cost is $O(\alpha mn)$; choosing
$\beta=\alpha$, we benefit from fast matrix multiplication, as the
cost drops to $O(\alpha^{\omega-2} mn)$, as claimed previously.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The divide-and-conquer algorithm}\label{ssec:MBA-C}

\begin{algorithm}[t]
  \LinesNumbered \DontPrintSemicolon
  \caption{Divide-And-Conquer algorithm $\DACp$ \label{algo:dac}}

  \Input{Generators $(\mG,\mH)$ of $\mA$, threshold $\nu_0$}
  \Output{$r=\rank(\mA)$, generators of $\mA^{(r)}_{0,0}{}^{-1}$}

  $\mn = \min(m,n), \ i = \lceil \mn/2 \rceil, \ (\mY^{(0)},\mZ^{(0)}) =
  (\mG,\mH)$\;

  \lIf{$\max(m,n) \leq \nu_0$}{\Return $\Iter(\mG,\mH, \alpha)$} 
  $(r_0,\mY^{(r_0,m-r_0)}_1,\mZ^{(r_0,n-r_0)}_1) = 
  \DACp(\mY^{(0,i)}_0, \mZ^{(0,i)}_0,  \nu_0)$\;\label{line:dac0}

  Recover Schur compl. gen.
  $(\mY^{(r_0,m-r_0)}_0,\mZ^{(r_0,n-r_0)}_0)=(\mY',\mZ')$ from
  $(\mY^{(r_0,m-r_0)}_1,\mZ^{(r_0,n-r_0)}_1)$ and
  $(\mY^{(0)},\mZ^{(0)})$
  \label{line:recoverfullgen0}\;

  \If{$r_0 < i$}{
    \lIf{Schur complement is non zero}{\Raise}\label{algo:testschur}
    \lElse{\Return $(r_0,\mY^{(r_0)}_0,\mZ^{(r_0)}_0)$ }}

  $(r_1,\mY^{(r_2,m-r_1)}_1,\mZ^{(r_2,n-r_1)}_1) =
  \DACp(\mY',\mZ', \nu_0), \ r_2=r_0+r_1$\label{line:dac1}\;


  \Return $r_2$ and $(\mY^{(r_2,m-r_2)}_1,\mZ^{(r_2,n-r_2)}_1)$
  computed from $(\mY^{(r_2,m-r_1)}_1,\mZ^{(r_2,n-r_1)}_1)$ and
  $(\mY^{(r_0)},\mZ^{(r_0)})$
  \label{line:recoverfullgen1}\;
\end{algorithm}


We finally review the divide-and-conquer approach to solving
Problem~\ref{pb:cauchy}. The algorithm proceed essentially
following~\cite{JeMo10}, with the minor difference that do not assume
$\mA$ invertible, and that we explicitly check if $\mA$ satisfies the
generic rank profile assumption.

\smallskip\noindent{\bf \small Termination and correction.}
Recursive calls to $\DACp$ on lines~\ref{line:dac0},~\ref{line:dac1}
will raise an error if the respective input matrices
${\mA^{(i)}_{0,0}}$ and its Schur complement
$\mA^{(i)}_{1,1} - \mA^{(i)}_{1,0} {\mA^{(i)}_{0,0}}{}^{-1}
\mA^{(i)}_{0,1}$
are not in generic position, so we assume that we are not in that
case. Then the correction proof is similar to that in Section~\ref{ssec:quad}.

\smallskip\noindent{\bf \small Complexity.} Computations of
lines~\ref{line:recoverfullgen0},~\ref{line:recoverfullgen1} are of
the same essence; using Eq.~\eqref{eq:Si+j} and \eqref{eq:Yi+j}, we
recover the full generators $(\mY^{(i+j)},\mZ^{(i+j)})$ of
$\mS^{(i+j)}$ if we know the generators
$(\mY^{(i+j,m-j)}_1,\mZ^{(i+j,m-j)}_1)$ of $\mS^{(i,j)}_{0,0}{}^{-1}$
and previous full generators $(\mY^{(i)},\mZ^{(i)})$ of $\mS^{(i)}$.
Altogether, this boils down to $O(1)$ multiplications of Cauchy-like
matrices (for which we have generators of length $\alpha$) 
by $\alpha$ vectors.

In general, this multiplication costs $O(\alpha^2 \M(\mx) \log(\mx))$
operations, with $\mx=\max(m,n)$. If $\vu$ and $\vv$ are chosen as
geometric progressions, the cost for the matrix-vectors multiplication
drops to $O(\alpha^2 \M(\mx))$, implying similar saving for the total
runtime. If $\vu$ and $\vv$ have the same ratio (if this is the case
at the top-level, this will remain the case for all recursive calls),
we saw that can save a further constant factor.  Finally, for large
values of $\alpha$, we can apply an algorithm given
in~\cite{BoJeMoSc16}, which reduces the cost to
$O(\alpha^{\omega-1} \M(\mx))$.

On line~\ref{algo:testschur}, the Schur complement is zero if and only
if $\mY'\, \mZ' = 0$. This is done by finding a minimal set of
independent rows in $\mZ'$ (this takes time
$O(\alpha^{\omega-1} \mx)$) and multiplying their transposes by
${\mY'}$ (there are at most $\alpha$ such rows, so this takes time
$O(\alpha^{\omega-1} \mx)$ as well).

Taking all recursive steps into account, the total cost of $\DACp$ is
withing a factor $\log(\mx)$ that of the Cauchy-like matrix-vectors
multiplication discussed above.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Experimental results}

We have implemented the algorithms described so far, together with all
subroutines they rely on, in a C++ library available at
\url{gitsomething}. Our implementation is based on Shoup's
NTL~\cite{Shoup95,NTL} version 10.3.0, and is dedicated to word-size
primes (NTL's \texttt{lzz\_p} class); the divide-and-conquer algorithm
of Subsection~\ref{ssec:MBA-C} actually requires FFT primes.  In all
that follows, timings are measured on an Intel i7-4790 CPU with 32 GB
RAM; only one thread is used throughout. 

The main goal of our experiments is to assess whether fast matrix
multiplication can bring practical improvements that reflect the
theoretical ones, what are reasonable crossover points between
iterative and divide-and-conquer algorithms, and what is the range of
applicability of these structured methods compared to dense linear
algebra.  Thus, we focused on comparisons between our own
implementations of the techniques seen so far, and did not attempt
comparisons with other systems.

The main building blocks we need are fast polynomial and matrix
computations. NTL already implements very efficient FFT-based
polynomial arithmetic; matrix multiplication over small fields (for $p
< 2^{23}$) is now extremely efficient, comparable to reference
implementations such as FFLAS-FFPACK~\cite{fflas-ffpack}. On top of
this, we implemented a fast polynomial matrix multiplication, that
relies on the cyclotomic TFT of~\cite{ArSc15}. 

We first discuss Problem~\ref{pb:cauchy}. Our first tests compare our
new $O(\alpha^{\omega-2} n^2)$ algorithm to the previous one, with
runtime $O(\alpha n^2)$. Except for very small values of $n$, say $n <
50$, for which the behavior fluctuates rapidly, we found that the new
algorithm becomes more efficient for rather small values of $\alpha$:
our crossover points are $\alpha=8$ or $9$ for primes less than
$2^{23}$, and $\alpha=13$ or $14$ for larger word-size primes. The
following graph shows the time ratio between the algorithm
of~\cite{Mouilleron08} and our algorithm, for $\alpha=30$; the larger
value of $p$ used here is the FFT prime $p=82705526964617217=7^2
2^{54}+1$. We see that for small primes, for which matrix
multiplication is very efficient, our algorithm brings a substantial
improvement.

\todo{move label to top left}

\includegraphics[width=7cm]{ratio-block-eschost-desktop.pdf}

We consider next the divide-and-conquer algorithm. The key factor for
the efficiency of this algorithm is the cost of multiplying an $n
\times n$ Cauchy-like matrix of displacement rank $\alpha$ by $\alpha$
vectors. We compare the approach of cost $O(\alpha^2 \M(n))$, for
vectors $\vu,\vv$ in geometric progression with the same ratio, to the
algorithm of~\cite{BoJeMoSc16}, with cost $O(\alpha^{\omega-1}
\M(n))$, with a view of determining for what values of $\alpha$ (if
any) the latter becomes useful. 

For the former algorithm, because we are able to cache several forward
FFTs, we found it slightly more advantageous to forego the cyclotomic
TFT and simply use NTL's FFT. As a consequence, the runtime of the
first algorithm displays the typical FFT staircaise behavior. This in turn
implies that as $n$ grows, the crossover value for $\alpha$
fluctuates, roughly between $30$ and $55$. The following graph shows
the time ratio between the algorithm of~\cite{BoJeMoSc16} and the
direct one, for $\alpha=60$; at best, the new algorithm wins by a
factor of 2. The results do not depend much on the nature of the prime
(since polynomial arithmetic is a significant part of the runtime, and
behaves essentially in the same manner, independently of $p$).

\includegraphics[width=7cm]{ratio-mul-matrix-eschost-desktop.pdf}

Taking these results into account, we determined empirically crossover
values $\nu_0(\alpha)$ to end the recursion in the divide-and-conquer algorithm
and switch to the iterative algorithm. We expect 
$\nu_0(\alpha)$ to grow with $\alpha$, since for a given $\alpha$, it behaves
like the solution of $\alpha n^2 = \alpha^2 \M(n) \log(n)$ (assuming
here we do not use fast linear algebra, for simplicity). The following
table reports some values for $\nu_0$ (obtained by searching for $\nu_0$
in increments of $100$).  The threshold is higher for small primes,
since such primes benefit more from our new iterative algorithm.

\begin{center}
{\small
\begin{tabular}{|c||c|c|c|c|c|c|c|}\hline
  $\alpha$ & 1 & 5 & 10 & 15 & 20 & 25 &30 \\\hline\hline
$p=65537$  & 400 &400 &500  &1000  &2000  &3300  &4200 \\\hline
$p\simeq 2^{59}$  & 200 & 400 & 400 & 700 & 1300 & 1600 & 2000\\\hline
\end{tabular}
}
\end{center}

These values being set, we can then show runtimes for solving
Problem~\ref{pb:cauchy} modulo both $p=65537$ and
$p=8827055\-26964617217$, for increasing values of $n$; we use
$\alpha=5$ and $\alpha=50$, and we also display the runtime of a dense
matrix inversion in the same size. For a very small displacement rank
such as $\alpha=5$, the runtime is essentially the same for these two
primes; with $\alpha=50$, we observe a difference between these two
primes, by a factor of up to 3. In any case, there is a clear gain
over dense methods.

\includegraphics[width=8cm]{large_n-eschost-desktop.pdf}

We also determined, for a given value of $n$, the crossover value
$\alpha_0$ above which dense linear algebra becomes faster than
structured methods.  The value $\alpha_0(n)$ grows quite regularly
with $n$, good approximations being $\alpha_0(n) \simeq 0.2 n$ (for $p
< 2^{23}$) and $\alpha_0(n) \simeq 0.25 n$ (for larger values of
$p$). This means that there is a wide range of inputs for which
structured methods can be of use.

Our solution of Problem~\ref{pb:mosaic} is a direct reduction to the
Cauchy-like case. Putting the problem into Cauchy form by means of the
formulae of Eq.~(\ref{eq:gentoetogencau}) accounts for a small
fraction of the total runtime: between 5\% and 10\% for small $\alpha$
(say $\alpha < 10$), and less than 5\% for larger values of $\alpha$,
in all instances we considered.  In the thousands of experiments we used
for our tests, for matrix sizes such as $n \ge 5000$ and small primes
such as $p=65537$, we observed some instances where the Cauchy matrix
did not have generic rank profile. This never happened for larger 
values of $p$, having more than $50$ bits.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Modular techniques}\label{sec:lifting}

We now address Problem~\ref{pb:mosaic} in the particular case where
$\K=\Q$; without loss of generality, we assume that the entries of
$\mT$ are integers. 

Several {\em modular algorithms} are available to solve dense linear
systems over $\Q$. A first option relies on the Chinese Remainder
Theorem, solving the system modulo several primes $p_1,p_2,\dots$
before reconstructing the solution, making sure to ensure consistency
of the modular solutions when ${\rm ker}(\mT)$ has dimension more than
$1$. Other approaches such as Dixon's algorithm~\cite{Dixon82}, Newton
iteration or divide-and-conquer algorithms use one prime $p$, and lift
the solution modulo powers of $p$.

Newton iteration for structured matrices goes back to Pan's
article~\cite{Pan92} (for Toeplitz-like matrices), and is explained in
detail in~\cite[Chapter~7]{Pan01}. To the best of our knowledge, the
other approaches mentioned above have not been discussed in the
literature on structured matrices. Our goal in this section is to
first briefly present some of these techniques and analyze their
complexity for the problem at hand; we will then discuss their
practical performance. We will highlight in particular the case of
algebraic approximants, for which we are able to obtain significant
improvements.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Solving square systems by lifting}

In this subsection, we are given a prime power $p^t$, an $n \times n$
matrix $\mA$ and a vector $\vb$, both with entries modulo $p^t$, and
we assume that $\mA$ is invertible modulo $p^t$ (we denote by $\mB$
its inverse). In what follows, we denote by $\I:\mathbb{N}
\to \mathbb{R}$ a function such that integers of bit size at most $d$
can be multiplied in $\I(d)$ bit operations; using FFT
techniques, we can take XXX. As in~\cite{GaGe13}, we assume that
$d\mapsto \I(d)/d$ is non-decreasing.  We discuss algorithms
that solve the equation $\mA \vx = \vb$ modulo $p^{t}$, where $t$ is a
power of 2, and we estimate their complexity when $\mA$ is a
structured matrix.

We take $\CA$ such that, for any $t$, given a vector $\vx$ with
entries defined modulo $p^t$, we can compute $\mA \vx \bmod p^t$ in
$O(\CA \I(t \log(p)))$ bit operations. Below, we will
assume that $\mA$ is Cauchy-like as in the previous section, and given
by generators of length $\alpha$, so that we can take $\CA \in
O(\alpha \M(n))$. We saw in the introduction that in some cases (for
the calculation of algebraic approximants, for instance), better
estimates are available.

We will focus on two approaches: a divide-and-conquer algorithm and
Newton iteration, which both feature a running time linear in the
target precision $t$ (the runtime of Dixon's algorithm is linear in
$t$ only when the entries of $\mA$ are assumed to be $O(1)$; if $\mA$
has arbitrary entries, Dixon's algorithm takes quadratic time in $t$
even using fast integer arithmetic). We start with the
divide-and-conquer approach. Although this idea is entirely natural,
we do not know of a suitable reference for it.  

\todo{What do you think of citing either BerthomieuLebreton'12 for
  general linear system solving doing this DAC, or at least my thesis
  that deals also with the case of structured matrices (just
  toeplitz/hankel in fact)}


\smallskip\noindent{\textbf{\DACQ($\mA$, $b$, $p$, $t$)}}:

\noindent{\bf 0.} If $t = 1$, return $\mB  \vb$ mod $p$

\noindent{\bf 1.} Compute $\vx_0 := $\DACQ($\mA$, $\vb$, $p$, $t/2$)

\noindent{\bf 2.} Compute
$\vr_0 := \left( \mA \vx_0 - \vb\right) \bmod p^t$ and
$\vr_1 := r_0/ p^{t/2}$

\noindent{\bf 3.} Compute $\vx_1 := $\DACQ($\mA$, $\vr_1$, $p$, $t/2$)

\noindent{\bf 4.} Return $\vx_0 - p^{t/2}  \vx_1$ mod $p^t$

We assume that we have obtained generators of length $\alpha$ for
$\mB$, for instance by applying the algorithm of the previous section.
Thus, at the leaves of the recursion, each product $\mB \vb$ mod $p$
can be compute using $O(\alpha \M(n) \I(\log(p)))$ bit
operations. Using our assumption on $\I$, the total runtime
is then $O(\CA \I(t\log(p)) \log(t) +\alpha \M(n)
\I(\log(p)) t)$ bit operations; if we assume $p=O(1)$, this
becomes $O(\CA \I(t) \log(t) +\alpha \M(n) t)$.  Using our
upper bounds on $\CA$, this simplifies further as $O(\alpha \M(n)
\I(t) \log(t))$.

%% \smallskip\noindent{\textbf{Dixon($\mA$, $b$, $p$, $t$)}}:
%% \noindent{\bf 0.} Compute $\mM := \mA^{-1}$ (mod $p$)
%% \noindent{\bf 1.} Compute $x_0 := \mM  b$ (mod $p$)
%% \noindent{\bf 2.} for $i \in [1, t-1]$:
%% \begin{itemize}
%% 	\item Compute $b := (b - \mA  x_{i-1}) \mathbin{/} p$ 
%% 	\item Compute $x_i := \mM  b$ (mod $p$)
%% \end{itemize}
%% \smallskip\smallskip\noindent{\bf 3.} Return $\sum_{i=0}^{t-1} p^i  x_i $

We turn next to Newton iteration. The matrix form of Newton iteration
computes the inverses $\mB_k=\mA^{-1} \bmod p^{2^k}$ by means of the
relation $\mB_{k+1} = 2\mB_k - \mB_k \mA \mB_k \bmod p^{2^{k+1}}$;
once they are known, we deduce the solution to our system by means
of a matrix-vector product.

Let $(\mG,\mH)$ be generators for $\mA$. Pan's insight was to use the
relation above to compute $(\mX,\mY)=(-\mB \mG,\mB^{t}\mH)$, which are
generators for $\mB$. Given $(\mX_k,\mY_k)=(\mX,\mY) \bmod p^{2^k}$,
we can reconstruct $\mB_k$, but to deduce $(\mX_{k+1},\mY_{k+1})$, we
actually have to multiply $\mG$ and $\mH$ by $\mB_{k+1}$ (or its
transpose). This is done by using the expression for $\mB_{k+1}$ 
above, which gives
\begin{align*}
\mX_{k+1} &= -(2\mB_k - \mB_k \mA \mB_k)\mG \bmod p^{2^{k+1}}\\
\mY_{k+1} &= -(2\mB_k - \mB_k \mA \mB_k)^t\, \mH \bmod p^{2^{k+1}}.
\end{align*}
This time, we are multiplying matrices $\mA$ and $\mB_k$ (and their
transposes) by matrices of size $n \times \alpha$, all computations
being done modulo $p^{2^k+1}$. Each multiplication by {\em one} vector
takes $O(\alpha \M(n) \I(2^k \log(p)))$ operations (even if
$\CA$ can be taken less than $O(\alpha \M(n))$, the multiplications
by $\mB_k$ remain a bottleneck). Since we have to multiply these
matrices by $\alpha$ vectors, taking all steps into account, we arrive
at a total of $O(\alpha^2 \M(n) \I(t))$ bit operations
(assuming $p=O(1)$ for simplicity). Using the algorithm
of~\cite{BoJeMoSc16}, we remark that this can be further reduced to
$O(\alpha^{\omega-1} \M(n) \I(t))$, but this improvement was
not implemented.

Altogether, because it computes (generators of) a whole inverse,
Newton iteration is slower by a factor $\alpha$ (or slightly less, if
we use~\cite{BoJeMoSc16}); on the other hand, it saves a factor of
$\log(t)$. This is similar to what one observed when comparing these
techniques for e.g. the solution of differential
equations~\cite{aaaa,bbb}.

Finally, it is worthwhile mentioning the cost of Chinese Remaindering
techniques in an analogue context: instead of computing the solution
modulo the $2^k$-th power of a single prime $p$, we might want to 
solve the system modulo $2^k$ primes of the same magnitude. 
If we assume that $\mA$ remains invertible with generic rank profile modulo 
all these primes, and all these primes are $O(1)$, the runtime
is now $O(\alpha^{\omega-1} \M(n) \log(n) t)$, using the results 
in the previous section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Solving Problem~\ref{pb:mosaic}}

The techniques above allow us to solve instances of
Problem~\ref{pb:mosaic}, that is, find nullspace elements for a mosaic
Toeplitz matrix $\mT$, but they would require the knowledge of a
maximal invertible minor of it. However, there is no guarantee that
$\mT$ possesses such a minor that would be mosaic Toeplitz as well,
and we do not know how to find it efficientlty. Hence, we rely on the
transformation to the Cauchy structure / regularization technique of
Section~\ref{sec:basics}.

Over $\Q$, this approach has a certain shortcoming. The output is a
vector $\vb$ of the form $\vb=\mW_{\vv}[(-\mA_r^{-1}\vc)^t ~
  \vc^t]^t$, with $\mA=\mV_\vu \mT \mW_{\vv}$, $\mA_r^{-1}$ a maximal
minor of it and $\vc$ a random vector. Due to the preconditionning,
the entries of $\vb$ are expected of large height (larger than what we
may expect for a solution of $\mT$).

When ${\rm ker}(\mT)$ has dimension $1$, we are not affected by this
issue. Indeed, in this case, all solutions are of the form $\lambda
\vb_0$, for some vector $\vb_0 \in \Z^n$ whose bit-size can be bounded
only in terms of $\mT$ (for instance by means of Siegel's
lemma). Hence, it suffices to compute the solution $\vb$ of the
regularized system modulo a large enough integer $N$ and normalize it
by setting one of its entries to $1$; this gives us the normalization
of $\vb_0 \bmod N$. In addition, $\mT$ having nullity $1$ also makes
Chinese Remainder techniques easy to apply, since after the
normalization above, one can simply apply the Chinese Remainder
Theorem to all entries of the modular solutions we computed.






\bibliographystyle{plain} {\scriptsize \bibliography{structured}}


\end{document}
